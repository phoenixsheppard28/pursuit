


## High level user flow: 

1. User sends POST request to /url endpoint 


2. Server hands off to celery worker queue
    - Job is queued for background processing, does not block subsequent API requests
    - Creates a placeholder SourcePage table row
    - returns a UUID that can be used to access the job result at a later time

3.  Celery task 
    - First places the original url into the **SourcePages** table, marked as **"PROCESSING"**
    - Scrapes all websites linked to from the original url
    - determines their relevance, keywords, filetype, and stores data in the **TargetPages** table
    - Once scraping is done, marks original url in SourcePages as **"COMPLETE"**
        - If an error occured, marks it as **"FAILED"**






## Technology

1.  ### Relevance Heuristic
    - Relevance score determined by ChatGPT API querries. In this example I use GPT 4.1 nano as it's the cheapest one i can use while prototyping, but other models can be used if desired
    - LLM is given, as contenxt, and example of a 10/10 relevance text and a 1/10 relevance text
    - these text are also generated by a seperate ChatGPT API query 
        - The user can also pass in example texts of their own 

2.  ### Scapy Scraper
    - High performance web crawling framework 
    - Is given a starting website (root node) and performs a **depth first search** on the link graph
    - The maximum depth is specified as 2 here, but it can be changed to whatever
    - In essence, it will visit every hyperlink on the root page, extract its main text using **[Trafilatura](https://trafilatura.readthedocs.io/en/latest/index.html)**, and feed that into ChatGPT to rank its relvance 
    
        

3. ### **Celery**
    - Distrubuted task queue that allows for asynchronous execution of tasks
    - In this implementation, **Redis** is used as the message broker, although something like **RabbitMQ, Kafka, or ZeroMQ** could be used for this as well
    - A request is placed in a queue until one of the celery workers is available and can pick it up 
    - This allows for the user to send upwards of thousands of website scraping requests to the url endpoint, and come back later once its finished
        - **This is assuming the scaling suggestions are implemented**



## Scaling Suggestions
1. ### Swap to PostgreSQL
    - SQlite is used here for simplicity, but the **high number of writes** needed for a production workload would require a more robust database like PostgreSQL
2. ### Increace # of Workers
    - Horizontally scale Celery by increacing the number of workers
    - Ideally, this can be done automatically based on the load experienced
3. ### 